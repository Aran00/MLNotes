---
title: "08. Class Imbalance"
date: 2016-08-18 21:08\\\\\\\\
---

[TOC]
### 类别不平衡问题
  如果不同类别的训练样例数目稍有差别,通常影响不大,但若差别很大,则会对学习过程造成困扰.例如有998个反例,但正例只有2个,那么学习方法只需返回一个永远将新样本预测为反例的学习器,就能达到99.8%的精度;然而这样的学习器往往没有价值,因为它不能预测出任何正例.
  
  类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况.不失一般性,本节假定正类样例较少,反类样例较多.
  
  一般分类器决策规则为
  
若$\frac{y}{1-y} > 1$ 则预测为正例


然而,当训练集中正、反例的数目不同时,令$m^+$表示正例数目,$m^-$表示反例数目,则观测几率是$\frac{m^+}{m^-}$. 由于我们通常假设训练集是真实样本总体的无偏 采样,因此观测几率就代表了真实几率.于是,只要分类器的预测几率髙于观测几率就应判定为正例,即

若$\frac{y}{1-y} > \frac{m^+}{m^-}$ 则预测为正例

这就需对其预测值进行调整,令
$$
\begin{equation}
\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^+}{m^-}
\end{equation}
$$
这就是类别不平衡学习的一个基本策略 — “再缩放”(rescaling)
再缩放的思想虽简单,但实际操作却并不平凡,主要因为“训练集是真实样本总体的无偏釆样”这个假设往往并不成立.现有技术大体上有三类做法:

- 欠采样(undersampling): 即去除一些反例使得正、反例数目接近,然后再进行学习; 若随机丢弃反例,可能丢失一些重要信息
- 过釆样(oversampling): 增加一些正例使得正、反例数目接近,然后再进行学习; 不能简单地对初始正例样本进行重复采样,否则会招致严重的过拟合;
- 阈值移动(threshold-moving): 将(4)式嵌入到其决策过程中

“再缩放”也是“代价敏感学习”(cost-sensitive learn­ing)的基础.在代价敏感学习中将$\frac{m^-}{m^+}$用$\frac{cost^+}{cost^-}$代替即可.